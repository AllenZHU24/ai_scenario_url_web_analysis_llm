#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLMè§„åˆ’æ¨¡å—
è´Ÿè´£è¯†åˆ«æ ¸å¿ƒé¡µé¢ç±»å‹å’Œæ¨èæ ¸å¿ƒURLs
"""

import json
import logging
import os
import re
import urllib.parse
import random
from typing import Dict, List, Tuple, Optional
from datetime import datetime
from openai import OpenAI
import pathlib


class LLMPlanner:
    """LLMè§„åˆ’å™¨"""
    
    def __init__(self, api_key: str, base_url: str = "https://openrouter.ai/api/v1"):
        """åˆå§‹åŒ–LLMè§„åˆ’å™¨"""
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url
        )
        
        self.logger = logging.getLogger(__name__)
        
        # æ¨¡å‹é…ç½®
        self.high_model = "google/gemini-2.5-pro"          # é«˜æ€§èƒ½æ¨¡å‹ï¼ˆæ ¸å¿ƒé¡µé¢ç±»å‹è¯†åˆ«ï¼‰
        self.medium_model = "openai/gpt-4o-mini"           # ä¸­ç­‰æ€§èƒ½æ¨¡å‹ï¼ˆURLæ¨èï¼‰
        
        # æ ¸å¿ƒé¡µé¢ç±»å‹å­˜å‚¨
        self.core_page_types = []
        self.core_page_types_nested = {}
        
        # ç¼–è¯‘åçš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self._compiled_patterns = None
        
        self.logger.info(f"ğŸš€ LLMè§„åˆ’å™¨å·²åˆå§‹åŒ–")
    
    def generate_core_page_types(self, year_links_map, company_url, expected_types=50):
        """ç”Ÿæˆæ ¸å¿ƒé¡µé¢ç±»å‹å¹¶ä¿å­˜"""
        self.logger.info(f"ğŸ§  å¼€å§‹è¯†åˆ«æ ¸å¿ƒé¡µé¢ç±»å‹...")
        
        # å‡†å¤‡è¾“å‡ºç›®å½•å’Œæ–‡ä»¶
        output_dir = os.path.join("outputs", company_url)
        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, f"{company_url}_core_page_types.json")
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç»“æœ
        if os.path.exists(output_file):
            self.logger.info(f"âœ… å‘ç°å·²å­˜åœ¨çš„æ ¸å¿ƒé¡µé¢ç±»å‹æ–‡ä»¶: {output_file}")
            self._load_core_page_types(output_file)
            return output_file
        
        # åˆå¹¶å¹¶æŠ½æ · URL
        merged = []
        for links in year_links_map.values():
            merged.extend(links)
        unique_links = list(dict.fromkeys(merged))  # ä¿åºå»é‡
        sample_links = unique_links[:]
        random.shuffle(sample_links)  # éšæœºæ‰“ä¹±é¡ºåº

        sample_block = "\n".join(sample_links)

        prompt = (
            f"You are a senior e-commerce website analyst. Below are internal URLs from multiple archive years of THE SAME website.\n\n"
            f"Please summarise â‰ˆ{expected_types} CORE PAGE TYPES covering the complete customer journey "
            f"(Awareness, Interest, Consideration, Decision, Fulfillment, Retention).\n\n"
            f"For better readability, structure the final JSON as *nested by stage*:\n"
            f"```json\n{{\n  \"Awareness Stage\": [\n    {{\n      \"type_name\": \"Home\",\n      \"typical_url_patterns\": [\"/\"]\n    }}\n  ],\n  \"Interest Stage\": [],\n  \"Consideration Stage\": [],\n  \"Decision Stage\": [],\n  \"Fulfillment Stage\": [],\n  \"Retention Stage\": []\n}}\n```\n"
            f"Explanation: keys are the six customer-journey stages; each value is an array of page-type objects containing *type_name* and *typical_url_patterns*. Do NOT include any other keys.\n\n"
            f"# Sample internal links (first {len(sample_links)})\n{sample_block}"
        )

        # è°ƒç”¨é«˜æ€§èƒ½LLM
        llm_resp = self._call_llm(prompt, model=self.high_model, task_tag="è¯†åˆ«æ ¸å¿ƒé¡µé¢ç±»å‹")
        parsed = self._parse_llm_response(llm_resp)

        # å¤„ç†LLMå“åº”å¹¶æ„å»ºæ•°æ®ç»“æ„
        self._process_core_page_types_response(parsed)
        
        # ä¿å­˜ç»“æœ
        result_data = {
            "generation_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "company_url": company_url,
            "core_page_types": self.core_page_types_nested or self.core_page_types,
            "total_types": len(self.core_page_types)
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"âœ… æ ¸å¿ƒé¡µé¢ç±»å‹è¯†åˆ«å®Œæˆï¼Œå…± {len(self.core_page_types)} ä¸ªç±»å‹å·²ä¿å­˜åˆ°: {output_file}")
        return output_file

    def _process_core_page_types_response(self, parsed: Dict):
        """å¤„ç†LLMå“åº”ï¼Œæ„å»ºæ ¸å¿ƒé¡µé¢ç±»å‹æ•°æ®ç»“æ„"""
        if "core_page_types" in parsed:
            # æ—§æ ¼å¼ï¼šå¹³é“ºåˆ—è¡¨
            flat_list = parsed["core_page_types"]
            nested: Dict[str, List[Dict]] = {}
            for item in flat_list:
                stage = item.get("related_journey_stage", "Unknown Stage")
                item_copy = item.copy()
                item_copy.pop("related_journey_stage", None)
                nested.setdefault(stage, []).append(item_copy)
            self.core_page_types = flat_list
            self.core_page_types_nested = nested
        else:
            # æ–°æ ¼å¼ï¼šåµŒå¥—ç»“æ„
            transformed: List[Dict] = []
            self.core_page_types_nested = {}
            for stage_name, type_list in parsed.items():
                if not isinstance(type_list, list):
                    continue
                for item in type_list:
                    if isinstance(item, dict):
                        item_cp = item.copy()
                        item_cp["related_journey_stage"] = stage_name
                        self.core_page_types_nested.setdefault(stage_name, []).append({k:v for k,v in item.items() if k!="related_journey_stage"})
                        transformed.append(item_cp)
            self.core_page_types = transformed
    
    def _load_core_page_types(self, file_path: str):
        """ä»æ–‡ä»¶åŠ è½½æ ¸å¿ƒé¡µé¢ç±»å‹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            nested = data.get("core_page_types", {})
            flat = []
            for stage, items in nested.items():
                for it in items:
                    item_cp = it.copy()
                    item_cp["related_journey_stage"] = stage
                    flat.append(item_cp)
            
            self.core_page_types_nested = nested
            self.core_page_types = flat
            self.logger.info(f"âœ… ä»æ–‡ä»¶åŠ è½½æ ¸å¿ƒé¡µé¢ç±»å‹: {len(flat)} ä¸ªç±»å‹")
            
        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½æ ¸å¿ƒé¡µé¢ç±»å‹å¤±è´¥: {e}")

    def generate_llm_planning(self, year_links_map: Dict[str, List[str]], company_url: str, crawl_num: int = 15) -> str:
        """
        ç”ŸæˆLLMè§„åˆ’ç»“æœå¹¶ä¿å­˜
        
        Args:
            year_links_map: å¹´ä»½åˆ°é“¾æ¥åˆ—è¡¨çš„æ˜ å°„
            company_url: å…¬å¸URLæ ‡è¯†ç¬¦
            crawl_num: æ¯å¹´æ¨èçš„æ ¸å¿ƒURLæ•°é‡
            
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        self.logger.info(f"ğŸ¤– å¼€å§‹ç”ŸæˆLLMè§„åˆ’...")
        
        # å‡†å¤‡è¾“å‡ºç›®å½•å’Œæ–‡ä»¶
        output_dir = os.path.join("outputs", company_url)
        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, f"{company_url}_llm_planning.json")
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç»“æœ
        existing_results = {}
        if os.path.exists(output_file):
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                existing_results = data.get("yearly_analysis_results", {})
                self.logger.info(f"âœ… åŠ è½½å·²å­˜åœ¨çš„è§„åˆ’ç»“æœ: {len(existing_results)} å¹´")
            except Exception as e:
                self.logger.warning(f"âš ï¸ åŠ è½½å·²å­˜åœ¨è§„åˆ’ç»“æœå¤±è´¥: {e}")
        
        # ç¡®ä¿æ ¸å¿ƒé¡µé¢ç±»å‹å·²åŠ è½½
        if not self.core_page_types:
            core_types_file = os.path.join(output_dir, f"{company_url}_core_page_types.json")
            if os.path.exists(core_types_file):
                self._load_core_page_types(core_types_file)
            else:
                raise RuntimeError("æ ¸å¿ƒé¡µé¢ç±»å‹æœªç”Ÿæˆï¼Œè¯·å…ˆè¿è¡Œ generate_core_page_types()")
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self._compile_core_type_patterns()
        
        # æŒ‰å¹´å¤„ç†
        yearly_analysis = existing_results.copy()
        
        for year, valid_links in year_links_map.items():
            if year in yearly_analysis:
                self.logger.info(f"â© {year} å¹´è§„åˆ’å·²å­˜åœ¨ï¼Œè·³è¿‡")
                continue
            
            self.logger.info(f"ğŸ” å¤„ç† {year} å¹´çš„è§„åˆ’...")
            
            try:
                # åˆ†ç±»å€™é€‰URLs
                classified_urls = self._classify_candidate_urls(year, valid_links)
                
                # LLMé€‰æ‹©æ ¸å¿ƒURLs
                llm_analysis = self._select_core_urls_from_classification(year, classified_urls, crawl_num)
                
                # ä¼˜åŒ–çˆ¬å–ç­–ç•¥
                final_analysis = self._optimize_crawl_strategy(llm_analysis, valid_links, year, enforce_in_valid=False)
                
                # åˆå¹¶classified_urlsä¸­çš„ä¿¡æ¯ï¼ˆæ–¹ä¾¿åç»­æ’æŸ¥ï¼‰
                final_analysis["classified_urls_count"] = len(classified_urls) 
                final_analysis["classified_urls"] = classified_urls

                yearly_analysis[year] = final_analysis
                
                self.logger.info(f"âœ… {year} å¹´è§„åˆ’å®Œæˆ: æ¨è {len(final_analysis.get('recommended_crawl_pages', []))} ä¸ªæ ¸å¿ƒURL")
                
                # å¢é‡å†™å…¥ã€æ–­ç‚¹ç»­è·‘
                self._write_planning_file(output_file, company_url, yearly_analysis)
                
            except Exception as e:
                self.logger.error(f"âŒ {year} å¹´è§„åˆ’å¤±è´¥: {e}")
                # å¤±è´¥æ—¶ä¸ä¿å­˜è¯¥å¹´ä»½ç»“æœï¼Œè®©åç»­è¿è¡Œé‡æ–°å°è¯•
        
        # æœ€ç»ˆæ±‡æ€»ä¿å­˜ï¼ˆå†æ¬¡å†™å…¥ï¼Œç¡®ä¿æ—¶é—´æˆ³ä¸ºæœ€ç»ˆå®Œæˆæ—¶é—´ï¼‰
        self._write_planning_file(output_file, company_url, yearly_analysis)
        total_urls = sum(len(data.get("recommended_crawl_pages", [])) for data in yearly_analysis.values())
        self.logger.info(f"ğŸ’¾ LLMè§„åˆ’å…¨éƒ¨å¹´ä»½å¤„ç†å®Œæˆï¼Œå…± {total_urls} ä¸ªæ¨èURLå·²ä¿å­˜åˆ°: {output_file}")
        
        return output_file
    
    def _compile_core_type_patterns(self):
        """ç¼–è¯‘æ ¸å¿ƒé¡µé¢ç±»å‹çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼"""
        if self._compiled_patterns:
            return

        # ç¡®ä¿æœ‰åµŒå¥—æ˜ å°„
        nested: Dict[str, List[Dict]] = self.core_page_types_nested
        if not nested:
            nested = {}
            for item in self.core_page_types:
                stage = item.get("related_journey_stage", "Unknown Stage")
                item_cp = item.copy()
                item_cp.pop("related_journey_stage", None)
                nested.setdefault(stage, []).append(item_cp)
            self.core_page_types_nested = nested

        compiled: List[Tuple[str, str, "re.Pattern", int]] = []

        for stage, type_list in nested.items():
            for type_obj in type_list:
                type_name = type_obj.get("type_name", "Unknown")
                for raw_pat in type_obj.get("typical_url_patterns", []):
                    # ç‰¹æ®Šå¤„ç†æ ¹è·¯å¾„æ¨¡å¼
                    if raw_pat.strip() == "/":
                        pat_regex = r"^/$"  # ä¸¥æ ¼åŒ¹é…æ ¹è·¯å¾„
                    else:
                        # è½¬æ¢é€šé…ç¬¦æ¨¡å¼ä¸ºæ­£åˆ™è¡¨è¾¾å¼
                        pat_regex = re.escape(raw_pat).replace(r"\\*", ".*")
                    
                    try:
                        regex = re.compile(pat_regex, re.IGNORECASE)
                    except re.error:
                        self.logger.debug(f"âš ï¸ æ— æ•ˆçš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ {raw_pat}ï¼Œè·³è¿‡")
                        continue
                    
                    compiled.append((stage, type_name, regex, len(raw_pat)))

        # æŒ‰æ¨¡å¼é•¿åº¦æ’åºï¼Œç¡®ä¿æ›´å…·ä½“çš„æ¨¡å¼ä¼˜å…ˆåŒ¹é…
        compiled.sort(key=lambda x: -x[3])
        self._compiled_patterns = compiled
    
    def _classify_candidate_urls(self, year: str, valid_links: List[str], homepage_url: str = "") -> List[Dict]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼åˆ†ç±»å€™é€‰URLs"""
        results: List[Dict] = []

        # è·å–ä¸»åŸŸåç”¨äºè¿‡æ»¤
        home_host = ""
        if homepage_url:
            home_host = self._get_home_host(homepage_url)

        for url in valid_links:
            # æå–çœŸå®ç«™ç‚¹è·¯å¾„
            real_part = url
            candidate_host = ""
            
            if "web.archive.org" in url:
                # ä»Wayback URLä¸­æå–åŸå§‹URL
                m = re.search(r"/web/\d+/(https?://.*)", url)
                if m:
                    underlying = m.group(1)
                    # ä¿®æ­£å¸¸è§é”™è¯¯
                    if underlying.startswith("http:/") and not underlying.startswith("http://"):
                        underlying = underlying.replace("http:/", "http://", 1)
                    if underlying.startswith("https:/") and not underlying.startswith("https://"):
                        underlying = underlying.replace("https:/", "https://", 1)
                    
                    parsed_under = urllib.parse.urlparse(underlying)
                    candidate_host = parsed_under.netloc.lower()
                    
                    # å»æ‰ç«¯å£å·å’Œwwwå‰ç¼€
                    if ":" in candidate_host:
                        candidate_host = candidate_host.split(":")[0]
                    if candidate_host.startswith("www."):
                        candidate_host = candidate_host[4:]
                    
                    # ä¸ºregexåŒ¹é…åšå‡†å¤‡
                    real_part = parsed_under.path
                    if real_part == "":
                        real_part = "/"
                    if parsed_under.query:
                        real_part += '?' + parsed_under.query
                # else:
                #     # Fall back to stripping manually
                #     real_part = re.sub(r"^https://web\.archive\.org/web/\d+/", "", url)
            else:
                parsed = urllib.parse.urlparse(url)
                candidate_host = parsed.netloc.lower()
                real_part = parsed.path
                if real_part == "":
                    real_part = "/"
                if parsed.query:
                    real_part += '?' + parsed.query

            # åŸŸåè¿‡æ»¤
            if home_host and candidate_host and candidate_host != home_host and candidate_host != f"www.{home_host}":
                continue

            # æ­£åˆ™åŒ¹é…
            matched = False
            for stage, type_name, regex, pattern_len in self._compiled_patterns:
                if regex.match(real_part):
                    results.append({
                        'url': url,
                        'customer_journey_stage': stage,
                        'type_name': type_name
                    })
                    matched = True
                    break

        self.logger.info(f"âœ… URLæ­£åˆ™åŒ¹é…å®Œæˆï¼Œå…±è¿‡æ»¤å‡º{len(results)}/{len(valid_links)}ä¸ªclassified_urls")
        return results
    
    def _get_home_host(self, homepage_url: str) -> str:
        """è·å–ä¸»é¡µå¯¹åº”çš„ä¸»åŸŸå"""
        parsed = urllib.parse.urlparse(homepage_url)
        host = parsed.netloc.lower()

        # å¤„ç†Wayback URL
        if 'web.archive.org' in host:
            m = re.search(r"/web/\d+/(https?://[^/]+)", homepage_url)
            if m:
                underlying = m.group(1)
                # ä¿®æ­£å¸¸è§é”™è¯¯
                if underlying.startswith("http:/") and not underlying.startswith("http://"):
                    underlying = underlying.replace("http:/", "http://", 1)
                if underlying.startswith("https:/") and not underlying.startswith("https://"):
                    underlying = underlying.replace("https:/", "https://", 1)               
                host = urllib.parse.urlparse(underlying).netloc.lower()

        # å»æ‰ç«¯å£å·å’Œwwwå‰ç¼€
        if ':' in host:
            host = host.split(':')[0]
        if host.startswith('www.'):
            host = host[4:]
        return host
    
    def _select_core_urls_from_classification(self, year: str, classified_urls: List[Dict], crawl_num: int = 15) -> Dict:
        """è°ƒç”¨LLMé€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„URLs"""
        classification_json = json.dumps(classified_urls, ensure_ascii=False, indent=2)

        prompt = (
            f"You are a highly precise e-commerce analysis engine. Based on the classified URL list below, select the most core and important {crawl_num} URLs.\n\n"
            f"<selection_rules>\n"
            # f"1.[IMPORTANT!!!] Final count **must be exactly {crawl_num}** (unless total classified URLs < {crawl_num}, then return all).\n"
            # f"1. Cover ALL six journey stages (Browse, Search, Decision-Making, Purchase, After-Sales, Customer Retention), and provide 2-3 URLs for EACH stage.\n"
            # f"2. Do NOT select more than two URLs having the SAME type_name.\n"
            f"1. Identify and prioritize URLs that are core, important, and valuable to the customer.\n"
            f"2. Cover ALL six journey stages (Awareness, Interest, Consideration, Decision, Fulfillment, Retention).\n"
            f"3. Ensure that type_name of the selected URLs are well-diversified and not concentrated.\n"
            f"4. Please choose English URLs only."
            # f"4. [IMPORTANT!!!] Final count **must be exactly {crawl_num}** (unless total classified URLs < {crawl_num}, then return all"
            f"</selection_rules>\n\n"
            f"<classified_urls total=\"{len(classified_urls)}\">\n```json\n{classification_json}\n```\n</classified_urls>\n\n"
            f"<output_format_instructions>Output ONLY the JSON object starting with '{{' and ending with '}}', using the schema: \n"
            f"{{\n  \"core_url_recommendations\": {{\n    \"recommended_url_list\": [\n      {{\n        \"url\": \"...\",\n        \"customer_journey_stage\": \"...\",\n        \"type_name\": \"...\",\n        \"selection_reason\": \"...\"\n      }}\n    ],\n    \"total_recommendations\": {crawl_num},\n    \"coverage_scenario_types\": [\"Awareness\", \"Interest\", \"Consideration\", \"Decision\", \"Fulfillment\", \"Retention\"]\n }}\n}}\n"
            f"</output_format_instructions>"
        )
        
        llm_resp = self._call_llm(prompt, model=self.medium_model, task_tag="é€‰æ‹©æ ¸å¿ƒURLs")
        parsed = self._parse_llm_response(llm_resp)
        return parsed
    
    def _optimize_crawl_strategy(self, llm_analysis: Dict, valid_content_urls: List[str], year: str, *, enforce_in_valid: Optional[bool] = None,) -> Dict:
        """Generate final crawl list.

        Args:
            enforce_in_valid: If provided, overrides self.enforce_recommended_in_valid_list for this call.
        """
        # ç›´æ¥æå–LLMæ¨èçš„æ ¸å¿ƒURLs
        recommended_urls = []
        core_url_data = llm_analysis.get("core_url_recommendations", {})

        enforce = True if enforce_in_valid is None else enforce_in_valid
        
        if core_url_data and "recommended_url_list" in core_url_data:
            for url_info in core_url_data["recommended_url_list"]:
                url_str = url_info["url"] if isinstance(url_info, dict) else url_info
                
                if enforce:
                    if url_str in valid_content_urls:
                        recommended_urls.append(url_str)
                else:
                    recommended_urls.append(url_str)
        
        # å»é‡ä¿æŒé¡ºåº
        recommended_urls = list(dict.fromkeys(recommended_urls))
        
        # ç»Ÿè®¡é‡å æ•°é‡
        overlap_count = sum(1 for u in recommended_urls if u in valid_content_urls)
        
        # å¦‚æœæ²¡æœ‰æ¨èURLï¼Œä½¿ç”¨å‰10ä¸ªæœ‰æ•ˆURL
        if not recommended_urls:
            recommended_urls = valid_content_urls[:10]
            self.logger.warning(f"LLMæ¨èURLsä¸åœ¨æœ‰æ•ˆåˆ—è¡¨ä¸­ï¼Œä½¿ç”¨å‰ {len(recommended_urls)} ä¸ªæœ‰æ•ˆURLs")
        
        self.logger.info(f"âœ… URLæ¨èå®Œæˆ: {len(recommended_urls)} ä¸ªæ¨èï¼Œ{overlap_count} ä¸ªåœ¨æœ‰æ•ˆé¡µé¢å†…")

        # æ„å»ºåˆ†æç»“æœ
        analysis_result = {
            "analysis_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "using_real_llm": True,
            "llm_model": self.medium_model,
            "year": year,
            "discovered_links_count": len(valid_content_urls),
            "valid_content_urls_count": len(valid_content_urls),
            "recommended_crawl_pages": recommended_urls,
            "recommended_pages_count": len(recommended_urls),
            "recommended_in_valid_overlap": overlap_count,
            "enforce_recommended_in_valid_list": enforce,
            "core_url_details": core_url_data
        }
        
        return analysis_result
    
    def _call_llm(self, prompt: str, model: Optional[str] = None, *, task_tag: Optional[str] = None) -> str:
        """è°ƒç”¨LLM API"""
        try:
            model_to_use = model if model else self.medium_model
            
            tag_txt = f" [{task_tag}]" if task_tag else ""
            self.logger.info(f"ğŸ¤– è°ƒç”¨LLMæ¨¡å‹{tag_txt}: {model_to_use}")
            
            response = self.client.chat.completions.create(
                model=model_to_use,
                messages=[
                    {"role": "system", "content": "You are a professional e-commerce website analysis expert, skilled in differentiated analysis for different years. Please respond in English and strictly follow the required JSON format."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=30000
            )
            
            response_content = response.choices[0].message.content
            self.logger.info(f"âœ… LLMå“åº”æ¥æ”¶æˆåŠŸ{tag_txt}")
            
            return response_content
            
        except Exception as e:
            self.logger.error(f"âŒ LLM APIè°ƒç”¨å¤±è´¥: {e}")
            raise e
    
    def _parse_llm_response(self, response: str) -> Dict:
        """è§£æLLMå“åº”"""
        try:
            # æå–JSON
            if "```json" in response:
                json_start = response.find("```json") + 7
                json_end = response.find("```", json_start)
                json_str = response[json_start:json_end].strip()
            else:
                # å°è¯•å®šä½å¯¹è±¡æˆ–æ•°ç»„
                obj_start = response.find("{")
                arr_start = response.find("[")

                if obj_start == -1 and arr_start == -1:
                    raise ValueError("å“åº”ä¸­æœªæ‰¾åˆ°JSON")

                # ä¼˜å…ˆå¤„ç†å¯¹è±¡
                if obj_start != -1 and (arr_start == -1 or obj_start < arr_start):
                    json_start = obj_start
                    json_end = response.rfind("}") + 1
                    json_str = response[json_start:json_end]
                else:
                    json_start = arr_start
                    json_end = response.rfind("]") + 1
                    json_str = response[json_start:json_end]
            
            parsed = json.loads(json_str)
            return parsed
            
        except (json.JSONDecodeError, ValueError) as e:
            self.logger.error(f"âŒ LLMå“åº”è§£æå¤±è´¥: {e}")
            raise Exception(f"LLMå“åº”æ ¼å¼é”™è¯¯ï¼Œæ— æ³•è§£æåˆ†æç»“æœ: {e}")

    def load_llm_planning(self, company_url: str) -> Dict[str, Dict]:
        """åŠ è½½å·²ä¿å­˜çš„LLMè§„åˆ’ç»“æœ"""
        output_dir = os.path.join("outputs", company_url)
        planning_file = os.path.join(output_dir, f"{company_url}_llm_planning.json")
        
        if not os.path.exists(planning_file):
            self.logger.error(f"âŒ è§„åˆ’æ–‡ä»¶ä¸å­˜åœ¨: {planning_file}")
            return {}
        
        try:
            with open(planning_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            yearly_results = data.get("yearly_analysis_results", {})
            self.logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(yearly_results)} å¹´çš„è§„åˆ’æ•°æ®")
            return yearly_results
            
        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½è§„åˆ’æ–‡ä»¶å¤±è´¥: {e}")
            return {}

    # ----------------- æ–°å¢å·¥å…·æ–¹æ³• -----------------
    def _write_planning_file(self, output_file: str, company_url: str, yearly_analysis: Dict[str, Dict]):
        """å°†è§„åˆ’ç»“æœå†™å…¥æ–‡ä»¶ï¼ˆç”¨äºå¢é‡ä¿å­˜ï¼‰"""
        try:
            result_data = {
                "agent_version": "LLM_Planner_v1.0",
                "company_url": company_url,
                "export_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "yearly_analysis_results": yearly_analysis
            }
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, ensure_ascii=False, indent=2)
            self.logger.debug(f"ğŸ’¾ å·²å¢é‡å†™å…¥è§„åˆ’æ–‡ä»¶: {output_file}ï¼ˆå…± {len(yearly_analysis)} å¹´ï¼‰")
        except Exception as e:
            self.logger.warning(f"âš ï¸ å¢é‡å†™å…¥è§„åˆ’æ–‡ä»¶å¤±è´¥: {e}")


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    api_key = "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"
    planner = LLMPlanner(api_key)
    print("LLMè§„åˆ’å™¨åˆå§‹åŒ–å®Œæˆ")
