#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åœºæ™¯åˆ†ææ¨¡å—
è´Ÿè´£åˆ†æç½‘é¡µå†…å®¹å¹¶è¯†åˆ«ç”¨æˆ·åœºæ™¯
"""

import json
import logging
import os
import re
import time
from typing import Dict, List, Set, Tuple, Optional
from datetime import datetime
import pathlib
from url_processing import URLProcessor


class ScenarioAnalyzer:
    """åœºæ™¯åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åœºæ™¯åˆ†æå™¨"""
        self.url_processor = URLProcessor()
        self.logger = logging.getLogger(__name__)
        
        # åœºæ™¯è¯†åˆ«ç»“æœå­˜å‚¨
        self.yearly_scenario_data = {}
        self.analysis_summary = {}
        
        # å¾®åœºæ™¯å®šä¹‰
        self.micro_scenarios = self._load_scenario_definitions()
        
        self.logger.info(f"ğŸ” åœºæ™¯åˆ†æå™¨å·²åˆå§‹åŒ–")
    
    def _load_scenario_definitions(self) -> Dict:
        """åŠ è½½å¾®åœºæ™¯å®šä¹‰"""
        try:
            base_dir = pathlib.Path(__file__).resolve().parent
            scenarios_file = base_dir / "inputs" / "1_scenario_mapping" / "micro_scenarios_definitions_v0.3.json"
            with open(scenarios_file, 'r', encoding='utf-8') as f:
                scenarios_data = json.load(f)
            
            self.logger.info(f"âœ… æˆåŠŸåŠ è½½ {scenarios_data.get('total_scenarios', 0)} ä¸ªå¾®åœºæ™¯")
            return scenarios_data["scenarios"]
            
        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½åœºæ™¯å®šä¹‰å¤±è´¥: {e}")
            return {}
    
    def analyze_scenarios_for_company(self, llm_planning_results: Dict[str, Dict], company_url: str) -> str:
        """
        åŸºäºLLMè§„åˆ’ç»“æœåˆ†æåœºæ™¯å¹¶ä¿å­˜
        
        Args:
            llm_planning_results: LLMè§„åˆ’ç»“æœï¼ŒåŒ…å«æ¯å¹´æ¨èçš„URLs
            company_url: å…¬å¸URLæ ‡è¯†ç¬¦
            
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        self.logger.info(f"ğŸ” å¼€å§‹åˆ†æåœºæ™¯: {company_url}")
        
        # å‡†å¤‡è¾“å‡ºç›®å½•
        output_dir = os.path.join("outputs", company_url)
        os.makedirs(output_dir, exist_ok=True)
        websites_dir = os.path.join(output_dir, "websites")
        os.makedirs(websites_dir, exist_ok=True)
        
        # è¾“å‡ºæ–‡ä»¶è·¯å¾„
        output_file = os.path.join(output_dir, f"{company_url}_scenarios.json")
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç»“æœ
        existing_results = {}
        if os.path.exists(output_file):
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                existing_results = data.get("yearly_detailed_results", {})
                self.logger.info(f"âœ… åŠ è½½å·²å­˜åœ¨çš„åœºæ™¯ç»“æœ: {len(existing_results)} å¹´")
            except Exception as e:
                self.logger.warning(f"âš ï¸ åŠ è½½å·²å­˜åœ¨åœºæ™¯ç»“æœå¤±è´¥: {e}")
        
        # æŒ‰å¹´åˆ†æåœºæ™¯
        self.yearly_scenario_data = existing_results.copy()
        
        for year, planning_data in llm_planning_results.items():
            # æ–­ç‚¹ç»­è·‘
            if year in self.yearly_scenario_data:
                self.logger.info(f"â© {year} å¹´åœºæ™¯å·²å­˜åœ¨ï¼Œè·³è¿‡")
                continue
            
            self.logger.info(f"ğŸ” åˆ†æ {year} å¹´çš„åœºæ™¯...")
            
            try:
                crawl_urls = planning_data.get("recommended_crawl_pages", [])
                if not crawl_urls:
                    self.logger.warning(f"âŒ {year} å¹´æ²¡æœ‰æ¨èURLï¼Œè·³è¿‡")
                    continue
                
                year_scenarios, successful_pages = self._analyze_scenarios_for_year(year, crawl_urls, websites_dir)
                
                self.yearly_scenario_data[year] = {
                    "identified_scenarios": list(year_scenarios),
                    "total_scenario_count": len(year_scenarios),
                    "analysis_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "llm_recommended_pages_count": len(crawl_urls),
                    "page_success_rate": f"{successful_pages}/{len(crawl_urls)}",
                    "stage_distribution": self._categorize_scenarios_by_stage(year_scenarios),
                }
                
                self.logger.info(f"âœ… {year} å¹´åœºæ™¯åˆ†æå®Œæˆ: {len(year_scenarios)} ä¸ªåœºæ™¯")
                
                # å¢é‡ä¿å­˜
                self._generate_analysis_summary(company_url)
                
            except Exception as e:
                self.logger.error(f"âŒ {year} å¹´åœºæ™¯åˆ†æå¤±è´¥: {e}")
                self.yearly_scenario_data[year] = {
                    "identified_scenarios": [],
                    "total_scenario_count": 0,
                    "error": str(e)
                }
        
        # ç”Ÿæˆå¹¶ä¿å­˜åˆ†ææ‘˜è¦
        output_file = self._generate_analysis_summary(company_url)
        
        return output_file
    
    def _analyze_scenarios_for_year(self, year: str, crawl_urls: List[str], websites_dir: str) -> Set[str]:
        """åˆ†æå•å¹´çš„åœºæ™¯"""
        year_scenarios: Set[str] = set()
        successful_pages = 0
        
        for i, url in enumerate(crawl_urls):
            self.logger.info(f"ğŸ“„ [{year}] åˆ†æé¡µé¢ {i + 1}/{len(crawl_urls)}")
            try:
                page_scenarios = self._identify_scenarios_in_page(url, year, websites_dir)
                if page_scenarios:
                    year_scenarios.update(page_scenarios)
                successful_pages += 1
            except Exception as e:
                self.logger.warning(f"âŒ é¡µé¢åˆ†æå¤±è´¥ {url}: {e}")
            
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        
        return year_scenarios, successful_pages
    
    def _identify_scenarios_in_page(self, url: str, year: str, websites_dir: str) -> Set[str]:
        """è¯†åˆ«å•ä¸ªé¡µé¢ä¸­çš„å¾®åœºæ™¯"""
        scenarios = set()
        
        try:
            # è·å–é¡µé¢å†…å®¹
            content = self.url_processor.get_page_content(url, max_length=None)
            if not content:
                return scenarios
            
            content_lower = content.lower()
            
            # ä¿å­˜å†…å®¹åˆ°txtæ–‡ä»¶
            self._save_content_to_txt(url, content_lower, year, websites_dir)
            
            # éå†æ‰€æœ‰å¾®åœºæ™¯å®šä¹‰è¿›è¡ŒåŒ¹é…
            for stage, stage_scenarios in self.micro_scenarios.items():
                for scenario_id, scenario_info in stage_scenarios.items():
                    scenario_name = scenario_info['name']
                    keywords = scenario_info['keywords']
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å…³é”®è¯åŒ¹é…
                    for keyword in keywords:
                        if keyword.lower() in content_lower:
                            scenarios.add(f"{scenario_id}_{scenario_name}")
                            break  # æ‰¾åˆ°åŒ¹é…ï¼Œç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªåœºæ™¯
            
            return scenarios
            
        except Exception as e:
            self.logger.warning(f"âŒ åœºæ™¯è¯†åˆ«å¤±è´¥ {url}: {e}")
            return scenarios
    
    def _save_content_to_txt(self, url: str, content_lower: str, year: str, websites_dir: str):
        """ä¿å­˜å†…å®¹åˆ°txtæ–‡ä»¶"""
        try:
            # ç”Ÿæˆæ—¶é—´æˆ³
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # åˆ›å»ºæ–‡ä»¶å
            filename = f"{year}_{timestamp}.txt"
            
            # å®Œæ•´æ–‡ä»¶è·¯å¾„
            file_path = os.path.join(websites_dir, filename)
            
            # å†™å…¥å†…å®¹åˆ°æ–‡ä»¶
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(f"æ¥æºURL: {url}\n")
                f.write("=" * 80 + "\n\n")
                f.write(content_lower)
            
            self.logger.info(f"ğŸ’¾ å†…å®¹å·²ä¿å­˜åˆ°: {file_path}")
            
        except Exception as e:
            self.logger.warning(f"âŒ ä¿å­˜å†…å®¹å¤±è´¥ {url}: {e}")
    
    def _categorize_scenarios_by_stage(self, scenarios: Set[str]) -> Dict[str, int]:
        """æŒ‰é˜¶æ®µåˆ†ç±»åœºæ™¯"""
        stage_count = {
            "Awareness Stage": 0,
            "Interest Stage": 0, 
            "Consideration Stage": 0,
            "Decision Stage": 0,
            "Fulfillment Stage": 0,
            "Retention Stage": 0
        }
        
        for scenario in scenarios:
            scenario_id = scenario.split('_')[0]
            if scenario_id.startswith('1.'):
                stage_count["Awareness Stage"] += 1
            elif scenario_id.startswith('2.'):
                stage_count["Interest Stage"] += 1
            elif scenario_id.startswith('3.'):
                stage_count["Consideration Stage"] += 1
            elif scenario_id.startswith('4.'):
                stage_count["Decision Stage"] += 1
            elif scenario_id.startswith('5.'):
                stage_count["Fulfillment Stage"] += 1
            elif scenario_id.startswith('6.'):
                stage_count["Retention Stage"] += 1
                
        return stage_count 
    
    def _generate_analysis_summary(self, company_url: str = None) -> str:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        if not self.yearly_scenario_data:
            return ""
        
        total_scenarios = sum(data.get("total_scenario_count", 0) for data in self.yearly_scenario_data.values())
        
        self.analysis_summary = {
            "analysis_version": "Scenario_Analyzer_v1.0",
            "company_url": company_url,
            "generation_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "focus": "Core scenario identification functionality",
            
            "core_statistics": {
                "analyzed_years_count": len(self.yearly_scenario_data),
                "total_identified_scenarios": total_scenarios,
                "average_scenarios_per_year": round(total_scenarios / len(self.yearly_scenario_data), 1) if self.yearly_scenario_data else 0
            },
            
            "yearly_detailed_results": self.yearly_scenario_data
        }
        
        # åŠ¨æ€ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        if company_url:
            output_dir = os.path.join("outputs", company_url)
            os.makedirs(output_dir, exist_ok=True)
            output_file = os.path.join(output_dir, f"{company_url}_scenarios.json")
        else:
            os.makedirs("outputs", exist_ok=True)
            output_file = os.path.join("outputs", "scenario_analysis_summary.json")
        
        # å¯¼å‡ºåˆ†ææ‘˜è¦
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_summary, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"âœ… åœºæ™¯åˆ†æç»“æœå·²å¯¼å‡ºåˆ°: {output_file}")
        return output_file
    
    def load_scenarios(self, company_url: str) -> Dict[str, Dict]:
        """åŠ è½½å·²ä¿å­˜çš„åœºæ™¯åˆ†æç»“æœ"""
        output_dir = os.path.join("outputs", company_url)
        scenarios_file = os.path.join(output_dir, f"{company_url}_scenarios.json")
        
        if not os.path.exists(scenarios_file):
            self.logger.error(f"âŒ åœºæ™¯æ–‡ä»¶ä¸å­˜åœ¨: {scenarios_file}")
            return {}
        
        try:
            with open(scenarios_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            yearly_results = data.get("yearly_detailed_results", {})
            self.logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(yearly_results)} å¹´çš„åœºæ™¯æ•°æ®")
            return yearly_results
            
        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½åœºæ™¯æ–‡ä»¶å¤±è´¥: {e}")
            return {}


def verify_scenario_definitions():
    """éªŒè¯åœºæ™¯å®šä¹‰å®Œæ•´æ€§"""
    print("ğŸ” éªŒè¯å¾®åœºæ™¯å®šä¹‰å®Œæ•´æ€§...")
    
    try:
        base_dir = pathlib.Path(__file__).resolve().parent
        scenarios_file = base_dir / "inputs" / "1_scenario_mapping" / "micro_scenarios_definitions_v0.3.json"
        with open(scenarios_file, 'r', encoding='utf-8') as f:
            scenarios_data = json.load(f)
        
        scenarios = scenarios_data["scenarios"]
        total_count = 0
        
        for stage, stage_scenarios in scenarios.items():
            count = len(stage_scenarios)
            print(f"  {stage}: {count} scenarios")
            total_count += count
        
        print(f"\nâœ… Total: {total_count} micro-scenarios")
        print(f"ğŸ“„ Loaded from: {scenarios_file}")
        
        # è·å–é¢„æœŸè®¡æ•°
        expected_count = scenarios_data.get("total_scenarios", 62)
        if total_count == expected_count:
            print("ğŸ‰ åœºæ™¯å®šä¹‰å®Œæ•´!")
            return True
        else:
            print(f"âŒ åœºæ™¯è®¡æ•°ä¸åŒ¹é…! åº”è¯¥æ˜¯ {expected_count}ï¼Œå®é™…æ˜¯ {total_count}")
            return False
            
    except Exception as e:
        print(f"âŒ éªŒè¯åœºæ™¯å®šä¹‰å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    analyzer = ScenarioAnalyzer()
    print("åœºæ™¯åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    verify_scenario_definitions() 