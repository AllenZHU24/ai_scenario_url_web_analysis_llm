#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
URLå¤„ç†æ¨¡å—
è´Ÿè´£URLå‘ç°ã€ç­›é€‰å’ŒæŒ‰å¹´ä¿å­˜
"""

import json
import logging
import os
import re
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from typing import Dict, List, Set, Optional, Tuple
from datetime import datetime


class URLProcessor:
    """URLå¤„ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–URLå¤„ç†å™¨"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # æ’é™¤çš„URLæ¨¡å¼
        self.exclude_patterns = [
            r'\.pdf$', r'\.jpg$', r'\.png$', r'\.gif$', r'\.css$', r'\.js$',
            r'/legal/', r'/privacy/', r'/terms/', r'/cookie',
            r'/sitemap', r'/robots', r'/favicon', r'mailto:', r'tel:',
            r'#', r'javascript:'
        ]
        
        self.logger = logging.getLogger(__name__)
    
    def process_urls_for_company(self, historical_urls: List[Tuple[str, str]], company_url: str) -> str:
        """
        å¤„ç†å…¬å¸çš„æ‰€æœ‰å†å²URLsï¼ŒæŒ‰å¹´ç­›é€‰å¹¶ä¿å­˜
        
        Args:
            historical_urls: [(year, url), ...] å†å²URLåˆ—è¡¨
            company_url: å…¬å¸URLæ ‡è¯†ç¬¦
            
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        self.logger.info(f"ğŸŒ å¼€å§‹å¤„ç†å…¬å¸URLs: {company_url}")
        
        # å‡†å¤‡è¾“å‡ºç›®å½•
        output_dir = os.path.join("outputs", company_url)
        os.makedirs(output_dir, exist_ok=True)
        
        # è¾“å‡ºæ–‡ä»¶è·¯å¾„
        output_file = os.path.join(output_dir, f"{company_url}_filtered_links.json")
        
        # åˆå§‹åŒ–æˆ–åŠ è½½å·²å­˜åœ¨ç»“æœï¼Œå®ç°å¢é‡"æ–­ç‚¹ç»­è·‘"
        year_links_map: Dict[str, List[str]] = {}
        if os.path.exists(output_file):
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    year_links_map = data.get("year_links_map", {})
                self.logger.info(
                    f"ğŸ”„ å‘ç°å·²å­˜åœ¨çš„é“¾æ¥æ–‡ä»¶: {output_file}ï¼Œå·²åŠ è½½ {len(year_links_map)} å¹´çš„æ•°æ®ï¼Œå°†è¿›è¡Œå¢é‡å¤„ç†"
                )
            except Exception as e:
                self.logger.warning(f"âš ï¸ è¯»å–å·²å­˜åœ¨é“¾æ¥æ–‡ä»¶å¤±è´¥ï¼Œå°†é‡æ–°ç”Ÿæˆ: {e}")
        
        # å¼€å§‹æŒ‰å¹´å¤„ç†URLsï¼ˆä»…å¤„ç†ç¼ºå¤±æˆ–ä¸ºç©ºçš„å¹´ä»½ï¼‰
        for year, homepage_url in historical_urls:
            # å¦‚æœè¯¥å¹´å·²ç»æœ‰éç©ºé“¾æ¥ï¼Œåˆ™è·³è¿‡ï¼Œå®ç°å¢é‡å¤„ç†
            if year in year_links_map and len(year_links_map[year]) > 0:
                self.logger.info(f"â­ï¸ è·³è¿‡ {year} å¹´ï¼Œå·²æœ‰ {len(year_links_map[year])} ä¸ªé“¾æ¥")
                continue

            self.logger.info(f"ğŸ” å¤„ç† {year} å¹´çš„URLs...")
            
            try:
                # å‘ç°å†…éƒ¨é“¾æ¥
                internal_links = self.discover_internal_links(homepage_url, max_links=10000)
                
                # è¿‡æ»¤æœ‰æ•ˆé“¾æ¥
                filtered_links = self._filter_valid_links(internal_links)
                
                year_links_map[year] = filtered_links
                
                self.logger.info(f"âœ… {year} å¹´å¤„ç†å®Œæˆ: {len(filtered_links)} ä¸ªæœ‰æ•ˆé“¾æ¥")
                
            except Exception as e:
                self.logger.error(f"âŒ {year} å¹´å¤„ç†å¤±è´¥: {e}")
                year_links_map[year] = []
        
        # ä¿å­˜ç»“æœ
        result_data = {
            "generation_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "company_url": company_url,
            "total_years": len(year_links_map),
            "year_links_map": year_links_map
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        
        total_links = sum(len(links) for links in year_links_map.values())
        self.logger.info(f"ğŸ’¾ é“¾æ¥å¤„ç†å®Œæˆï¼Œå…± {total_links} ä¸ªé“¾æ¥å·²ä¿å­˜åˆ°: {output_file}")
        
        return output_file
    
    def discover_internal_links(self, homepage_url: str, max_links: int = 10000) -> List[str]:
        """
        ä»ä¸»é¡µå‘ç°å†…éƒ¨é“¾æ¥
        
        Args:
            homepage_url: ä¸»é¡µURL
            max_links: æœ€å¤§é“¾æ¥æ•°é‡
            
        Returns:
            å†…éƒ¨é“¾æ¥åˆ—è¡¨
        """
        self.logger.info(f"ğŸ” å¼€å§‹å‘ç°URL: {homepage_url}")
        
        try:
            # è·å–ä¸»é¡µå†…å®¹
            response = self.session.get(homepage_url, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            home_host = self._extract_home_host(homepage_url)
            discovered_links = set([homepage_url])  # åŒ…å«ä¸»é¡µæœ¬èº«
            
            # æå–æ‰€æœ‰é“¾æ¥
            for a_tag in soup.find_all(['a', 'area'], href=True):
                href = a_tag.get('href')
                if not href:
                    continue
                
                # è½¬æ¢ä¸ºå®Œæ•´URL
                full_url = urljoin(homepage_url, href)
                
                # ä¿®å¤ urljoin åœ¨ Wayback URL ä¸­çš„é—®é¢˜
                if "/web/" in full_url and "http:/" in full_url and "http://" not in full_url:
                    full_url = full_url.replace("http:/", "http://")
                if "/web/" in full_url and "https:/" in full_url and "https://" not in full_url:
                    full_url = full_url.replace("https:/", "https://")
                
                # æå–å€™é€‰é“¾æ¥çš„çœŸå®ä¸»åŸŸå
                candidate_host = self._extract_home_host(full_url)

                # è¿‡æ»¤åŒä¸»åŸŸå
                if candidate_host == "" or candidate_host == home_host:
                    # æ¸…ç†URL
                    clean_url = full_url.split("#")[0]  # ç§»é™¤ fragment
                    
                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥åŒ…å«
                    if self._should_include_url(clean_url):
                        discovered_links.add(clean_url)
                        
                        if len(discovered_links) >= max_links:
                            break
            
            result = list(discovered_links)
            self.logger.info(f"âœ… å‘ç° {len(result)} ä¸ªå†…éƒ¨é“¾æ¥")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ å‘ç°URLæ—¶å‡ºé”™: {e}")
            return [homepage_url]  # è‡³å°‘è¿”å›ä¸»é¡µ
    
    def _extract_home_host(self, url: str) -> str:
        """æå–ä¸»é¡µå¯¹åº”çš„ä¸»åŸŸåï¼ˆå»é™¤ www. å‰ç¼€ï¼‰"""
        parsed = urlparse(url)
        host = parsed.netloc.lower()
        
        # Wayback åœºæ™¯: host == web.archive.orgï¼Œéœ€è¦æå–çœŸå®ç«™ç‚¹åŸŸå
        if "web.archive.org" in host:
            m = re.search(r"/web/\d+/(https?://[^/]+)", url)
            if m:
                underlying = m.group(1)
                # ä¿®æ­£å¸¸è§çš„ http:/ã€https:/ é”™è¯¯
                if underlying.startswith("http:/") and not underlying.startswith("http://"):
                    underlying = underlying.replace("http:/", "http://", 1)
                if underlying.startswith("https:/") and not underlying.startswith("https://"):
                    underlying = underlying.replace("https:/", "https://", 1)
                host = urlparse(underlying).netloc.lower()
        
        # å»æ‰ç«¯å£å·
        if ":" in host:
            host = host.split(":")[0]
        # å»æ‰ leading 'www.'
        if host.startswith("www."):
            host = host[4:]
        return host
    
    def _should_include_url(self, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åŒ…å«è¯¥URL"""
        url_lower = url.lower()
        
        # æ’é™¤ä¸éœ€è¦çš„URL
        for pattern in self.exclude_patterns:
            if re.search(pattern, url_lower):
                return False
        
        return True
    
    def _filter_valid_links(self, links: List[str]) -> List[str]:
        """è¿‡æ»¤æœ‰æ•ˆé“¾æ¥ï¼šåªè¿‡æ»¤æŠ€æœ¯æ–‡ä»¶ï¼Œä¸è¿›è¡Œå†…å®¹æ£€æŸ¥"""
        valid_links: List[str] = []

        for link in links:
            if not self._is_meaningful_url(link):
                self.logger.debug(f"ğŸš« [Structure] {link}")
                continue
        
            # if not self._is_url_reachable(link):
            #     self.logger.debug(f"ğŸš« [Unreachable] {link}")
            #     continue

            valid_links.append(link)
            self.logger.debug(f"âœ… Valid URL: {link}")

        self.logger.info(
            f"âœ… URLè¿‡æ»¤å®Œæˆ: {len(valid_links)}/{len(links)} é“¾æ¥é€šè¿‡ç»“æ„è¿‡æ»¤"
        )
        return valid_links

    def _is_meaningful_url(self, url: str) -> bool:
        """åŸºäºURLç»“æ„åˆ¤æ–­æ˜¯å¦æœ‰æ„ä¹‰ï¼ˆè¿‡æ»¤æŠ€æœ¯æ–‡ä»¶ï¼‰"""
        # è¿‡æ»¤æ˜æ˜¾çš„æŠ€æœ¯æ–‡ä»¶å’Œé™æ€èµ„æº
        excluded_extensions = ['.css', '.js', '.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico', 
                             '.pdf', '.xml', '.json', '.txt', '.zip', '.woff', '.ttf']
        excluded_paths = ['/static/', '/assets/', '/css/', '/js/', '/images/', '/img/', 
                         '/fonts/', '/media/', '/resources/', '/ajax/', '/api/']
        
        url_lower = url.lower()
        
        # æ’é™¤æŠ€æœ¯æ–‡ä»¶æ‰©å±•å
        if any(url_lower.endswith(ext) for ext in excluded_extensions):
            return False
        
        # æ’é™¤æŠ€æœ¯è·¯å¾„
        if any(path in url_lower for path in excluded_paths):
            return False
        
        return True

    def _is_url_reachable(self, url: str) -> bool:
        """å¿«é€Ÿåˆ¤æ–­ URL æ˜¯å¦å¯è®¿é—®ä¸”éå…¸å‹é”™è¯¯é¡µã€‚
        ä»…ä¸‹è½½å°‘é‡ HTMLï¼›è‹¥çŠ¶æ€ç ä¸åœ¨ 200-399 èŒƒå›´æˆ–å‘½ä¸­ Wayback å…¸å‹é”™è¯¯æç¤ºï¼Œåˆ™åˆ¤å®šä¸ºä¸å¯ç”¨ã€‚"""

        try:
            import requests  # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…é¡¶å±‚ä¾èµ–

            resp = self.url_discovery.session.get(url, timeout=10, allow_redirects=True, stream=True)
            if resp.status_code >= 400:
                return False

            # è¯»å–å‰ 2KB æ–‡æœ¬ç”¨äºé”™è¯¯æ£€æµ‹
            snippet = resp.text[:2048].lower()

            wayback_errors = [
                "got an http", "response at crawl time", "redirecting to", "impatient?",
                "page cannot be crawled", "not in archive", "page not found", "robots.txt",
                "this content is not available", "excluded from the wayback machine", "calendar not available"
            ]

            hits = sum(1 for kw in wayback_errors if kw in snippet)
            return hits < 2

        except Exception:
            return False

    def get_page_content(self, url: str, max_length: Optional[int] = 3000) -> Optional[str]:
        """è·å–é¡µé¢å†…å®¹ç”¨äºåˆ†æ"""
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
            for script in soup(["script", "style"]):
                script.decompose()
            
            # è·å–ä¸»è¦æ–‡æœ¬å†…å®¹
            text = soup.get_text()
            # æ¸…ç†æ–‡æœ¬
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            # æ ¹æ® max_length é™åˆ¶æ–‡æœ¬é•¿åº¦
            if max_length is not None and len(text) > max_length:
                text = text[:max_length] + "..."
            
            return text
            
        except Exception as e:
            self.logger.warning(f"âŒ è·å–é¡µé¢å†…å®¹å¤±è´¥ {url}: {e}")
            return None

    def load_filtered_links(self, company_url: str) -> Dict[str, List[str]]:
        """åŠ è½½å·²ä¿å­˜çš„è¿‡æ»¤é“¾æ¥"""
        output_dir = os.path.join("outputs", company_url)
        links_file = os.path.join(output_dir, f"{company_url}_filtered_links.json")
        
        if not os.path.exists(links_file):
            self.logger.error(f"âŒ é“¾æ¥æ–‡ä»¶ä¸å­˜åœ¨: {links_file}")
            return {}
        
        try:
            with open(links_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            year_links_map = data.get("year_links_map", {})
            self.logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(year_links_map)} å¹´çš„é“¾æ¥æ•°æ®")
            return year_links_map
            
        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½é“¾æ¥æ–‡ä»¶å¤±è´¥: {e}")
            return {}


def extract_company_url_from_filepath(file_path: str) -> str:
    """ä»æ–‡ä»¶è·¯å¾„ä¸­æå–å…¬å¸URL"""
    import os
    # è·å–æ–‡ä»¶åï¼ˆå»é™¤è·¯å¾„å’Œæ‰©å±•åï¼‰
    filename = os.path.basename(file_path)
    # å»é™¤æ‰©å±•å
    company_url = os.path.splitext(filename)[0]
    return company_url


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    processor = URLProcessor()
    test_url = "https://web.archive.org/web/20241231234159/https://www.apple.com/"
    
    links = processor.discover_internal_links(test_url, max_links=10)
    print(f"å‘ç°çš„é“¾æ¥æ•°: {len(links)}")
    
    for i, link in enumerate(links[:5], 1):
        print(f"{i}. {link}") 